import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, precision_recall_curve

model = pd.read_csv("Obfuscated-MalMem2022.csv")
model.columns
model['Class'] = model.Class.map({'Benign': 0, 'Malware': 1})

X = model.drop(['Category'], axis=1)
Y = model["Class"]

# Shuffle data
model = model.sample(frac=1).reset_index(drop=True)

# Create training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Set the random seed for reproducibility
tf.random.set_seed(42)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=["accuracy"]
)

history = model.fit(X_train, y_train, epochs=5)

model_1 = model.evaluate(X_test, y_test)
model_1

# Plot loss curve
plt.plot(history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()

# Plot accuracy curve
plt.plot(history.history['accuracy'])
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy')
plt.show()

# Obtain predicted probabilities for the test data
predicted_probabilities = model.predict(X_test)

# Convert predicted probabilities to class labels using a threshold
threshold = 0.5  # Adjust the threshold based on your specific problem
predicted_labels = tf.where(predicted_probabilities > threshold, 1, 0)

# Calculate the confusion matrix
cm = tf.math.confusion_matrix(y_test, predicted_labels)

# Define class labels
class_labels = ['Benign', 'Malware']

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks(ticks=[0.5, 1.5], labels=class_labels)
plt.yticks(ticks=[0.5, 1.5], labels=class_labels)
plt.show()

# Compute false positive rate (FPR), true positive rate (TPR), and thresholds
fpr, tpr, thresholds = roc_curve(y_test, predicted_probabilities)

# Compute Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='ROC Curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line indicating random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Calculate precision and recall
precision, recall, thresholds = precision_recall_curve(y_test, predicted_probabilities)

# Plot precision-recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)
plt.show()

# Calculate class distribution
class_counts = [len(y_train[y_train == 0]), len(y_train[y_train == 1])]

# Plot class distribution bar chart
plt.figure(figsize=(8, 6))
plt.bar(['Class 0', 'Class 1'], class_counts)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Distribution')
plt.show()
